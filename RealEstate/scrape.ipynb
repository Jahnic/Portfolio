{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Scraping through Chrome webdriver\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting URLs\n",
    "centris = \"https://www.centris.ca/en/properties~for-sale?view=Thumbnail\"\n",
    "duproprio = \"https://duproprio.com/en/search/list?search=true&is_for_sale=1&with_builders=1&parent=1&pageNumber=1&sort=-published_at\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Centris:\n",
    "    \"\"\"\n",
    "    This class represents a chrome webdriver object with access to centris.ca.\n",
    "    \n",
    "    Attr:\n",
    "    self.url - starting url for scraping process\n",
    "    self.data - pandas.DataFrame object containing scraped data\n",
    "    self.driver - Chrome webdriver\n",
    "    self.DRIVER_PATH - path to Chrome webdriver\n",
    "    self.old_DOM - web elements found in previous DOM \n",
    "    \"\"\"\n",
    "     \n",
    "    def __init__(self, url=\"https://www.centris.ca/en/houses~for-sale~lac-simon/11851081?view=Summary&uc=3\"): \n",
    "        self.url = url\n",
    "        self.data = pd.DataFrame()\n",
    "        # Path to Chromedriver\n",
    "        self.DRIVER_PATH = 'C:/webdriver/chromedriver.exe'\n",
    "        self.driver = None\n",
    "        # Verification of new DOM\n",
    "        self.old_DOM = {\\\n",
    "                        'title' : [],\\\n",
    "                        'address' : [],\\\n",
    "                        'price' : [],\\\n",
    "                        'lat' : [],\\\n",
    "                        'long' : [],\\\n",
    "                        'descriptions' : [],\\\n",
    "                        'neighbourhood_top' : [],\\\n",
    "                        'neighbourhood_mid' : [],\\\n",
    "                        'neighbourhood_buttom' : [],\\\n",
    "                        'demographics_buttons' : [],\\\n",
    "                    }\n",
    "        \n",
    "    def reset_old_DOM(self):\n",
    "        self.old_DOM = {\\\n",
    "                        'title' : [],\\\n",
    "                        'address' : [],\\\n",
    "                        'price' : [],\\\n",
    "                        'lat' : [],\\\n",
    "                        'long' : [],\\\n",
    "                        'descriptions' : [],\\\n",
    "                        'neighbourhood_top' : [],\\\n",
    "                        'neighbourhood_mid' : [],\\\n",
    "                        'neighbourhood_buttom' : [],\\\n",
    "                        'demographics_buttons' : [],\\\n",
    "                    }\n",
    "\n",
    "    def append_data(self, title, address, price,\\\n",
    "            lat, long, descriptions, neighbourhood_indicators,\\\n",
    "            population, demographics):\n",
    "        \"\"\"Appends new data to existing data frame.\n",
    "        \n",
    "        Args:\n",
    "        title - string\n",
    "        address - string \n",
    "        price - \n",
    "        lat - \n",
    "        long - \n",
    "        descriptions - \n",
    "        neighbourhood_indicators -\n",
    "        population - \n",
    "        demographics - \n",
    "        \"\"\"\n",
    "        new_data = pd.DataFrame({\\\n",
    "                        'title': title,\\\n",
    "                        'address': address,\\\n",
    "                        'price': price,\\\n",
    "                        'lat': lat,\\\n",
    "                        'long': long\\\n",
    "                    }, index=[0])\n",
    "        \n",
    "       \n",
    "#       description_table = pd.DataFrame()\n",
    "#         headers_of_interest = [\\\n",
    "#                 \"rooms\", \"bedrooms\", \"powder room\", \"Number of units\", \"Building style\",\\\n",
    "#                 \"Condominium type\", \"Year built\", \"Building area\", \"Lot area\", \"walk_score\",\\\n",
    "#                 \"Net area\", \"Parking (total)\", \"Main unit\", \"Potential gross revenue\", \"Pool\"\\\n",
    "#                               ]\n",
    "#         # Ensures consistency accross listings\n",
    "#         for header in headers_of_interest:\n",
    "#             if header in descriptions.keys():\n",
    "#                 value = descriptions[header]\n",
    "#             else:\n",
    "#                 value = np.nan\n",
    "#             description_table[header] = pd.Series(value)\n",
    "\n",
    "        # DESCRIPTIONS\n",
    "        for key in descriptions.keys():\n",
    "            header = key\n",
    "            value = descriptions[header]\n",
    "            description_table[header] = pd.Series(value)\n",
    "        \n",
    "        # POPULATION AND DEMOGRAPHICS\n",
    "        new_data = pd.concat([new_data, neighbourhood_indicators, description_table,\\\n",
    "                             population, demographics], axis=1)\n",
    "        # LOGGING --------------------------     \n",
    "        #print(new_data)\n",
    "        \n",
    "        self.data = self.data.append(new_data, sort=False,\\\n",
    "                                     ignore_index=True)\n",
    "        \n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "        \n",
    "    def start_driver(self):\n",
    "        \"\"\"\n",
    "        Starts and returns Crome webdriver. \n",
    "        The page link in the url attribute \n",
    "        is opened in headless mode.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Activate headless mode for fastest response\n",
    "        options = Options()\n",
    "        options.add_argument('--start-maximized') # open Browser in maximized mode\n",
    "        options.add_argument('--incognito')\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\"); # overcome limited resource problems\n",
    "        \n",
    "        # Start driver with url\n",
    "        self.driver = webdriver.Chrome(executable_path=self.DRIVER_PATH)\n",
    "        self.driver.get(self.url)\n",
    "\n",
    "    def sort_listings(self):\n",
    "        \"\"\"Sorts listings in webdriver from newest to oldest.\"\"\"\n",
    "        \n",
    "        # Click drop down menu\n",
    "        drop_down = self.driver.find_element_by_xpath(\\\n",
    "                                    \"//button[@id='dropdownSort']\")\n",
    "        drop_down.click()\n",
    "        \n",
    "        # Sort by most recent listings\n",
    "        sort_by = self.driver.find_element_by_xpath(\"//a[@data-option-value='3']\")\n",
    "        sort_by.click()\n",
    "    \n",
    "    def goto_first_page(self):\n",
    "        try:\n",
    "            next_page = self.driver.find_element_by_xpath(\\\n",
    "                                        \"//li[@class='goFirst']\")\n",
    "            next_page.click()\n",
    "        except:\n",
    "            print(\"goFirst button not available\")\n",
    "    \n",
    "    def next_page(self):\n",
    "        try:\n",
    "            next_page = self.driver.find_element_by_xpath(\\\n",
    "                                        \"//li[@class='next']\")\n",
    "            next_page.click()\n",
    "            pass\n",
    "        except:\n",
    "            time.sleep(0.5)\n",
    "            # Try again after waiting 0.5 sec.\n",
    "            try:\n",
    "                next_page = self.driver.find_element_by_xpath(\\\n",
    "                                            \"//li[@class='next']\")\n",
    "                next_page.click()\n",
    "                pass\n",
    "            except:\n",
    "                print(\"Next-page button not found!\")\n",
    "                \n",
    "    def get_page_position(self):\n",
    "        '''Returns the first and last page of the current search.\n",
    "        \n",
    "        Returns\n",
    "        tuple - (current_page, last_page), '''\n",
    "        \n",
    "        pages = self.driver.find_element_by_xpath(\\\n",
    "                                    \"//li[@class='pager-current']\").text.\\\n",
    "                                    split(\" / \")\n",
    "        \n",
    "        current_page, last_page = (int(page.replace(\",\",\"\")) for page in pages)\n",
    "        \n",
    "        return (current_page, last_page)\n",
    "    \n",
    "    def refresh_page(self):\n",
    "        \"Refreshes current webdriver page.\"\n",
    "        self.driver.refresh()\n",
    "        print(\"Page is being refreshed.\")\n",
    "        # Wait until page fully loaded\n",
    "        time.sleep(2)\n",
    "        \n",
    "    def distance(origin, destination):\n",
    "        \"\"\"Calculates distances from latitudinal/longitudinal data using\n",
    "        the haversine formula\"\"\"\n",
    "        lat1, lon1 = origin\n",
    "        lat2, lon2 = destination\n",
    "        radius = 6371 # km\n",
    "        \n",
    "        #Convert from degrees to radians\n",
    "        dlat = math.radians(lat2-lat1)\n",
    "        dlon = math.radians(lon2-lon1)\n",
    "        \n",
    "        # Haversine formula\n",
    "        a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) \\\n",
    "            * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "        d = radius * c\n",
    "\n",
    "        return d\n",
    "                \n",
    "                                                 \n",
    "# Instantiate class object\n",
    "centris = Centris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions need to be outside of the Class. wait_for_xpath() determined the approptiate time to call get_data(). Initially, both fuctions were part of the class object. It seems that after the get_data() call, the driver does not get updated within the class. This leads in some cases to old DOM's being accessed after the browser has already switched to the next page. To circumvent this issue, elements are called outside the class and tried until accessible. This allows the entire new DOM to be loaded before get_data() is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def wait_for_xpath(xpath: str, old_element):\n",
    "        \"\"\"\n",
    "        Wait until elements in new DOM are accessible.\n",
    "        \n",
    "        Arg.\n",
    "        xpath - xpath to new element \n",
    "        old_element - element at xpath from previous DOM (found in centris.old_DOM)\n",
    "        \n",
    "        Returns:\n",
    "        current_element - the element found in the new DOM at xpath\n",
    "        \"\"\"\n",
    "        \n",
    "        centris_driver = centris.driver\n",
    "        element_at_xpath = []\n",
    "        \n",
    "        # Ensure that the NEW rather than the previous or no DOM is active\n",
    "        # Maximum wait time 10 sec.\n",
    "        time_passed = 0\n",
    "        while (\\\n",
    "            (element_at_xpath == old_element or  element_at_xpath == [])\\\n",
    "            and (time_passed <= 10)\\\n",
    "              ):\n",
    "            # Wait for DOM to load\n",
    "            time.sleep(0.2)\n",
    "            time_passed += 0.2\n",
    "            \n",
    "            # Print every 2 seconds\n",
    "            if time_passed%2 == 0:\n",
    "                print(\"Waiting for new DOM...\")\n",
    "            \n",
    "            # Attempt to load new DOM\n",
    "            try: \n",
    "                element_at_xpath = centris_driver.find_elements_by_xpath(xpath)\n",
    "            except: pass\n",
    "        \n",
    "        # After 10 seconds unlikely to load at all -> restart entire process\n",
    "        if time_passed > 10:\n",
    "            print(\"RuntimeError: element not found.\")\n",
    "            centris.refresh_page()\n",
    "            get_data_from_centris()\n",
    "            wait_for_xpath(xpath, old_element)\n",
    "            \n",
    "        return element_at_xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_description(old_DOM):\n",
    "    \"\"\" Requires instantiated centris object. Scrapes and returns\n",
    "    description data: Year build, price, Net area, etc.\"\"\"\n",
    "    \n",
    "    descriptions = wait_for_xpath(\"//div[@class='col-lg-12 description']\",\\\n",
    "                                 old_DOM)\n",
    "    #First three elements not relevant\n",
    "    descriptions_list = descriptions[0].text.split(\"\\n\")[3:]\n",
    "    \n",
    "    #LOGGING------------------------\n",
    "    #print(\"DESCRIPTION:\", descriptions_list)\n",
    "    \n",
    "    # Update old_DOM dictionary with new element for next verification\n",
    "    centris.old_DOM['descriptions'] = descriptions\n",
    "    \n",
    "    return extract_descriptions(descriptions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data_dict found on this part of the page is inconsistent across listings\n",
    "    # The first row may contain the number of rooms, bedrooms and bathrooms without headers or may be missing\n",
    "    # Following rows have heathers with associated values after a line break\n",
    "    # The very last element may be a walking score without header\n",
    "    # Listings without first row may supply first row information in subsequent rows with headers\n",
    "    # Because of these inconsistencies, two seperate extractions need to be implemented: one for\n",
    "    # first row lements (if they exist) and another for subsequent rows\n",
    "\n",
    "def extract_descriptions(descriptions_list):\n",
    "    \"\"\"Takes in data from scrape_description() and returns it \n",
    "    as a dictionary\"\"\"\n",
    "    \n",
    "    # Transformed data\n",
    "    data_dict = {}\n",
    "    # Distinguish between elements from first and subsequent rows if first row exists\n",
    "    first_row = True\n",
    "    # Starting point for second part of transformation\n",
    "    second_row_index = 0\n",
    "    \n",
    "    # First Part\n",
    "    while first_row == True:\n",
    "        for description in descriptions_list:\n",
    "            numeric = re.findall(\"\\(*[0-9]+\\)*\", description) # numbers\n",
    "            text = re.findall(\"[A-Za-z]+[A-Za-z\\s\\-]*\", description) # text after/inbetween numbers \n",
    "\n",
    "            # Initial elements with numeric values correspond to first row\n",
    "            if (numeric != []):\n",
    "                # For each value there must be one text description\n",
    "                if (len(numeric) == len(text)):\n",
    "                    for description,value in zip(text, numeric):\n",
    "                        # Save as column in data_dict\n",
    "                        description_clean = description.replace(\"and\", \"\").strip()\n",
    "                        data_dict[description_clean] = value\n",
    "                    second_row_index += 1 \n",
    "                else:\n",
    "                    print(\"Unequal number of first row keys and values!\")\n",
    "                    print(\"Numbers:\", numeric)\n",
    "                    print(\"Text:\", text)\n",
    "                    break\n",
    "            else:\n",
    "                first_row = False # No numeric information implies header\n",
    "                break\n",
    "    \n",
    "    # Index range of second extraction\n",
    "    # Headers are found at every second index (0,2,4,...)\n",
    "    # Values are one index apart from their corresponding header (1,3,5,...)\n",
    "    list_length = len(descriptions_list)\n",
    "    if (list_length - second_row_index)%2 == 1: # Implies presence of element without header -> Walk Score\n",
    "        walk_score_listed = True\n",
    "        end_point = list_length -1\n",
    "    else:\n",
    "        walk_score_listed = False\n",
    "        end_point = list_length\n",
    "    # Indices corresponding to headers\n",
    "    extraction_range = range(second_row_index, end_point, 2)\n",
    "    \n",
    "    #LOGGING----------------------\n",
    "#     print(\"Second row index:\", second_row_index)\n",
    "#     print(\"Extraction range:\", extraction_range)\n",
    "#     print(\"List:\", descriptions_list)\n",
    "    \n",
    "    # Second Part\n",
    "    for header_index in extraction_range:\n",
    "        # Headers as column names\n",
    "        header = descriptions_list[header_index]\n",
    "        # Values corresponding to headers are found at subsequent indices\n",
    "        information = descriptions_list[header_index + 1] \n",
    "        data_dict[header] = information\n",
    "    \n",
    "    if walk_score_listed:\n",
    "        data_dict[\"walk_score\"] = descriptions_list[-1]\n",
    "        #LOGGING----------------------\n",
    "        #print(\"Walk Score:\", descriptions_list[-1])\n",
    "        \n",
    "    #LOGGING--------------------------\n",
    "#     print(\"Descriptions:\", data_dict)\n",
    "        \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_neighbourhood(old_DOM_top, old_DOM_mid, old_DOM_buttom):\n",
    "    \"\"\" Scrapes and returns a list of ratings \n",
    "    between 0-10 for a set of neighborhood indicators\n",
    "    such as groceries, parks, noise, etc.)\n",
    "    \"\"\"\n",
    "    driver = centris.driver\n",
    "    \n",
    "    # Extract elements from top section of scrollable list\n",
    "    neighbourhood_top = wait_for_xpath(\\\n",
    "                            \"//div[@class='ll-list ps ps--active-y']\",\\\n",
    "                            old_DOM_top)\n",
    "    # Split into indicators and ranking values\n",
    "    top = [x.text for x in neighbourhood_top][0].split(\"\\n\")\n",
    "    \n",
    "    # LOGGING----------------------\n",
    "#     print(\"Top neighbourhood:\", top)\n",
    "    \n",
    "    # Extract middle section - only one element\n",
    "    # Scroll and activate scrollable bar container\n",
    "    scrollable_bar = driver.find_element_by_xpath(\\\n",
    "                                            \"//div[@class='ps__thumb-y']\")\n",
    "    ActionChains(driver).\\\n",
    "        move_to_element(scrollable_bar).\\\n",
    "        send_keys(Keys.PAGE_DOWN).\\\n",
    "        click(scrollable_bar).perform()\n",
    "\n",
    "    # Elements from buttom of scrollable list\n",
    "    neighbourhood_mid = wait_for_xpath(\\\n",
    "                            \"//div[@class='ll-list ps ps--active-y']\",\\\n",
    "                            old_DOM_mid)\n",
    "    # Split into indicators and ranking values\n",
    "    \n",
    "    # LOGGING----------------------\n",
    "    #print(\"Neighbourhoud mid section:\", neighbourhood_mid)\n",
    "    \n",
    "    middle = [x.text for x in neighbourhood_mid][0].split(\"\\n\")\n",
    "    \n",
    "    # Extract buttom section\n",
    "    # Scroll and load remaining elements\n",
    "#     scrollable_bar = driver.find_element_by_xpath(\\\n",
    "#                                             \"//div[@class='ps__thumb-y']\")\n",
    "    ActionChains(driver).\\\n",
    "        move_to_element(scrollable_bar).\\\n",
    "        send_keys(Keys.PAGE_DOWN).\\\n",
    "        click(scrollable_bar).perform()\n",
    "    \n",
    "    # Elements from buttom of scrollable list\n",
    "    neighbourhood_buttom = wait_for_xpath(\\\n",
    "                            \"//div[@class='ll-list ps ps--active-y']\",\\\n",
    "                            old_DOM_buttom)\n",
    "    # Split into indicators and ranking values\n",
    "    buttom = [x.text for x in neighbourhood_buttom][0].split(\"\\n\")\n",
    "    \n",
    "    # LOGGING----------------------\n",
    "#     print(\"Buttom neighbourhood:\", buttom)\n",
    "    \n",
    "    # Unite all three sections by storing tuples of indicator names and corresponding values\n",
    "    united_list = []\n",
    "    list_length = len(top)\n",
    "    for i in range(0, list_length, 2):\n",
    "        united_list.append((top[i], top[i+1]))\n",
    "        united_list.append((middle[i], middle[i+1]))\n",
    "        united_list.append((buttom[i], buttom[i+1]))\n",
    "    \n",
    "    # Create set of unique tuples\n",
    "    neighbourhood_indicators = set(united_list)\n",
    "    \n",
    "    # LOGGING----------------------\n",
    "#     print(\"Number of neighborhood indicators: \", len(neighbourhood_indicators))\n",
    "#     print(\"UNITED:\", united_list)\n",
    "#     print(\"SET:\", neighbourhood_indicators)\n",
    "    \n",
    "    # Verify size and extract information as list\n",
    "    # If size unexpected, refresh page and restart process\n",
    "    if len(neighbourhood_indicators) < 8:\n",
    "            centris.refresh_page()\n",
    "            scrape_neighbourhood(old_DOM_top, old_DOM_buttom)\n",
    "\n",
    "    # Update old_DOM dictionary with new elements for next verification\n",
    "    centris.old_DOM['neighbourhood_top'] = neighbourhood_top\n",
    "    centris.old_DOM['neighbourhood_mid'] = neighbourhood_mid\n",
    "    centris.old_DOM['neighbourhood_buttom'] = neighbourhood_buttom\n",
    "\n",
    "    return extract_neighbourhood_indicators(neighbourhood_indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_neighbourhood_indicators(indicators):\n",
    "    \"\"\"Takes in neighbourhood data from scrape_neighbourhood() and returns it \n",
    "    in tabular form as a DataFrame object\"\"\"\n",
    "    data = pd.DataFrame()\n",
    "    for indicator in indicators:\n",
    "        header = indicator[0]\n",
    "        value = indicator[1]\n",
    "        data[header] = pd.Series(value)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_population():\n",
    "    \"\"\"Scrapes and returns population summary data (density, variation etc.)\"\"\"\n",
    "    population_summaries =  centris.driver.find_element_by_id('info')\n",
    "    population_summaries_list = population_summaries\\\n",
    "                        .text.split(\"\\n\")\n",
    "    \n",
    "    # LOGGING-----------------------\n",
    "    #print(\"Population:\", population_summaries_list)\n",
    "    \n",
    "    return extract_population(population_summaries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_population(population):\n",
    "    \"\"\"Takes in population data from scrape_population() and returns it \n",
    "    in tabular form as a DataFrame object\"\"\"\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    for info in population:\n",
    "        units_removed =  info.replace(\"hab/km2\", \"\").strip()\n",
    "        # Numeric data\n",
    "        numeric = re.findall(\"[0-9]+[0-9,]*\", units_removed)\n",
    "        numeric_clean = numeric[-1].replace(\",\",\"\")\n",
    "\n",
    "        # Text data for column names\n",
    "        header = re.findall(\"[a-zA-Z\\s]+\", units_removed)\n",
    "        header_clean = header[0]\n",
    "        # Add numeric data to header excluding the value at index -1\n",
    "        for numeric_head_data in numeric[:-1]:\n",
    "            header_clean = header_clean + str(numeric_head_data) + \" \"\n",
    "\n",
    "        data[header_clean] = pd.Series(numeric_clean).astype(\"int\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_demographics(old_DOM):\n",
    "    \"\"\"Scrapes and return demographic data found in a clickable list\"\"\"\n",
    "    \n",
    "    driver = centris.driver\n",
    "    # Clickable list containing demographic data\n",
    "    menu = driver.find_element_by_id(\"menu\")\n",
    "    # Load menu by moving browser to it\n",
    "    ActionChains(driver).\\\n",
    "    move_to_element(menu).perform()\n",
    "    \n",
    "    #Buttons to access demographics data (education, incomes, etc.)\n",
    "    demographics_buttons = wait_for_xpath(\\\n",
    "                        \"//div[@class='centrisSocioDemobutton']\",\\\n",
    "                                                 old_DOM)\n",
    "\n",
    "    # LOGGING------------------------\n",
    "    # print(\"DEMO. BUTTONS:\", demographics_buttons)\n",
    "\n",
    "    # First entry on clickable demographics list (pre-selected)\n",
    "    demographics = []\n",
    "\n",
    "    # Click buttons to access next demogrpahics elements\n",
    "    for button in demographics_buttons:\n",
    "        try: \n",
    "            button.click()\n",
    "        except: \n",
    "            print(\"Demographics button missing!\")\n",
    "            # Reattempt loading buttons\n",
    "            centris.refresh_page()\n",
    "            ActionChains(driver).\\\n",
    "            move_to_element(menu).\\\n",
    "            perform()\n",
    "            time.sleep(2) # extra time to load\n",
    "            demographics_buttons = wait_for_xpath(\\\n",
    "                        \"//div[@class='centrisSocioDemobutton']\",\\\n",
    "                                                 old_DOM)\n",
    "            \n",
    "        # Get and append data after button click\n",
    "        demographic_data = driver.find_element_by_class_name(\\\n",
    "                         \"socioDemoLabel\")\n",
    "        demographics.append(demographic_data.text)\n",
    "    \n",
    "    # Split each demographic component into separate list\n",
    "    # Example: splits \"Occupation\" data into -> [\"Owners\", \"35%\", \"Renters\", \"65%\"]\n",
    "    demographics = [demo.split(\"\\n\") for demo in demographics]\n",
    "    \n",
    "    #LOGGING------------------------\n",
    "#     print(\"DEMO. DATA:\", demographics)\n",
    "#     print(\"-\"*50)\n",
    "    \n",
    "    # Update old_DOM dictionary with new elements for next verification\n",
    "    centris.old_DOM['demographics_buttons'] = demographics_buttons\n",
    "    \n",
    "    return extract_demographics(demographics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_demographics(demographics):\n",
    "    \"\"\"Takes in demographic data from extract_demographics() and returns it \n",
    "    in tabular form as a DataFrame object\"\"\"\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for demographic in demographics:\n",
    "        # Remove empty stings from splitting double line breaks \\n\\n\n",
    "        removed_empty_strings = [x for x in demographic if x != \"\"]\n",
    "        # Format of demographic: [header, value, header, value, ...]\n",
    "        header_index = range(0, len(demographic), 2)\n",
    "        for i in header_index:\n",
    "            header = demographic[i] + \" (%)\" # add units to column names\n",
    "            value = demographic[i+1].replace(\"%\", \"\") # remove units from values \n",
    "            data[header] = pd.Series(value).astype(\"int\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_centris():\n",
    "        \"\"\"\n",
    "        Requires instantiate Centris object. Scrapes information from the\n",
    "        webdriver and appends it to the Centris object.\n",
    "        \"\"\"\n",
    "        driver = centris.driver\n",
    "        old_DOM = centris.old_DOM\n",
    "        \n",
    "        # Data from headers\n",
    "        print(\"Start scraping new page...\")\n",
    "        title = wait_for_xpath(\"//span[@data-id='PageTitle']\", old_DOM['title'])\n",
    "        address = wait_for_xpath(\"//h2[@itemprop='address']\", old_DOM['address'])\n",
    "        price = wait_for_xpath(\"//span[@itemprop='price']\", old_DOM['price'])\n",
    "        lat = wait_for_xpath(\"//meta[@itemprop='latitude']\", old_DOM['lat'])\n",
    "        long = wait_for_xpath(\"//meta[@itemprop='longitude']\", old_DOM['long'])\n",
    "        \n",
    "        # Save elements as old DOM\n",
    "        centris.old_DOM['title'] = title\n",
    "        centris.old_DOM['address'] = address\n",
    "        centris.old_DOM['lat'] = lat\n",
    "        centris.old_DOM['long'] = long\n",
    "        \n",
    "        # Scrape remaining elements and store in dataframe\n",
    "        descriptions = scrape_description(old_DOM['descriptions'])\n",
    "        neighbourhood_indicators = scrape_neighbourhood(old_DOM['neighbourhood_top'],\\\n",
    "                                                            old_DOM['neighbourhood_mid'],\\\n",
    "                                                            old_DOM['neighbourhood_buttom'])\n",
    "        population = scrape_population()\n",
    "        demographics = scrape_demographics(old_DOM['demographics_buttons'])\n",
    "                \n",
    "        # Unify data in single dataframe and append to results table\n",
    "        centris.append_data(\n",
    "            title[0].text,\\\n",
    "            address[0].text,\\\n",
    "            price[0].text,\\\n",
    "            lat[0].get_attribute(\"content\"),\\\n",
    "            long[0].get_attribute(\"content\"),\\\n",
    "            descriptions,\\\n",
    "            neighbourhood_indicators,\\\n",
    "            population,\\\n",
    "            demographics\\\n",
    "        )\n",
    "        \n",
    "        # LOGGING--------------------------\n",
    "        #print(\"GET DATA: DESCRIPTIONS:\", descriptions)\n",
    "        \n",
    "        # Return to top of page, to access next-page button\n",
    "        body = driver.find_element_by_tag_name(\"body\")\n",
    "        body.send_keys(Keys.HOME)\n",
    "#         for i in range(7):\n",
    "#             body.send_keys(Keys.PAGE_UP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "centris = Centris()\n",
    "start = time.time()\n",
    "centris.start_driver()\n",
    "centris.sort_listings()\n",
    "print(\"Execution time:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the next cell, search for the region(s) you want to scrape in the webdriver window.\n",
    "This is not required but will narrow results and reduce runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time() \n",
    "current_page, last_page = centris.get_page_position() \n",
    "pages_to_scrape = last_page - current_page + 1 # in case scraping is interupted\n",
    "one_to_100 = range(1,100) # to print message each 1% completion\n",
    "\n",
    "print(\"Scraping initiated.\")\n",
    "print(\"Total number of pages to scrape:\", pages_to_scrape)\n",
    "print(\"Estimated runtime:\", round(pages_to_scrape*((9.6)/(60*60)), 2), \"hours\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i in range(pages_to_scrape):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"Page:\", i+1)\n",
    "    time_passed = 0 # to exit while loop after 10 seconds\n",
    "    \n",
    "    #Refresh every 20 pages to clear memory build-up\n",
    "    if (i+1)%20 == 0:\n",
    "        print(\"Clearing memory\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        # Each refresh frees some memory. Four seem to work best.\n",
    "        for i in range(4):\n",
    "            centris.refresh_page()\n",
    "            # Extra time for last refresh\n",
    "            # Ensures that DOM is fully loaded\n",
    "            if i == 3:\n",
    "                time.sleep(2)\n",
    "            \n",
    "    #Retrieve data    \n",
    "    get_data_from_centris()\n",
    "    \n",
    "    # Short delay for chrome to respond to PAGE_UP command\n",
    "    time.sleep(0.5)\n",
    "    centris.next_page()\n",
    "    \n",
    "    # Percent completed of scraping \n",
    "    percent_complete = round(100*((i)/total_pages),2)      \n",
    "    # Print after every 1% mark\n",
    "    if percent_complete in one_to_100:\n",
    "        execution_time = (time.time() - start)/(i+1) # seconds per page\n",
    "        print(percent_complete, \"%\", \"completed\")\n",
    "        print(\"Average execution time per page:\", round(execution_time, 2), \"sec.\")\n",
    "        print(\"Estimated remaining runtime:\", round(\\\n",
    "                                (total_pages - (i+1))\\\n",
    "                                *(execution_time\\\n",
    "                                /(60*60)), 1\\\n",
    "                                                   ), \"hours <\", \"-\"*50)\n",
    "        print(\"=\"*50)\n",
    "\n",
    "\n",
    "print(\"Total runtime:\", execution_time/(60*60), \"hours\")\n",
    "centris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "centris.data.to_csv(\"centris_montreal_complete.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}